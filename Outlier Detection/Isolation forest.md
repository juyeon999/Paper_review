# Isolation Forest
ğŸ’¡**Original Paperì˜ `anomaly score`ë¥¼ ì–´ë–»ê²Œ êµ¬í•˜ëŠ”ì§€ ì‚´í´ë³´ê³ , `sklearn.ensemble.IsolationForest`ì˜ `score_samples()` and `decision_function()`ë¡œ `anomaly score` êµ¬í•´ë³´ê¸°**ğŸ”¥
  
#Outlier detection methods #unsupervise
- Original Paper: Liu, Fei Tony, Kai Ming Ting, and Zhi-Hua Zhou. "Isolation forest." 2008 eighth ieee international conference on data mining. IEEE, 2008.
- paper: https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4781136&casa_token=C-cPukAgOK4AAAAA:TZzyoAuwhBlDAcMhK0C6QX7bd0tJCg55h8DVRjR-h0_ENVmT4DKCy-5E02OZWI5NUEOmQEN4XA&tag=1
- python: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html#sklearn.ensemble.IsolationForest.decision_function

## Original paper
### Anomaly Score
- iTreesì˜ PathLength $h(x)$ì˜ í‰ê· ì„ $c(n)$ë¡œ normalizeí•œ ê°’ì„ ì‚¬ìš©í•˜ëŠ”ë°,
- 0~1ë¡œ í‘œí˜„í•˜ê³  ì‹¶ì–´ì„œ anomaly score of x given n $s(x, n) = 2^{(sth)}$ìœ¼ë¡œ ì •ì˜í•¨
  <img width="503" alt="image" src="https://github.com/juyeon999/Paper_review/assets/132811616/058a99db-f27a-4197-9fab-048932195777">
  <img width="506" alt="image" src="https://github.com/juyeon999/Paper_review/assets/132811616/755f1c73-5c40-4aea-af17-fd682e2d2e7a">

**êµ¬ë¶„ ë°©ë²•**  
1ê³¼ ê°€ê¹Œìš¸ìˆ˜ë¡ outlier, 0ê³¼ ê°€ê¹Œìš¸ìˆ˜ë¡ inlier, 0.5 ê¸°ì¤€ìœ¼ë¡œ ë‚˜ëˆ„ê²Œ ë˜ì–´ìˆìŒ (contamination ê³ ë ¤í•´ì§„ ë¶„í¬ì— ëŒ€í•´ 0.5ë¡œ ë‚˜ëˆ„ë©´ ë¨)
  
## `sklearn.ensemble.IsolationForest` ì¤‘ì‹¬ìœ¼ë¡œ
### score_samples(X)
Original paperì˜ `anomaly score`ì˜ opposite(ìŒìˆ˜) ë²„ì „
**êµ¬ë¶„ ë°©ë²•**  
-1ê³¼ ê°€ê¹Œìš¸ìˆ˜ë¡ outlier, 0ê³¼ ê°€ê¹Œìš¸ìˆ˜ë¡ inlier, -0.5 ê¸°ì¤€ìœ¼ë¡œ ë‚˜ëˆ”

### decision_function(X)
Original paperì˜ anomaly scoreì˜ ë¶„í¬ë¥¼ ê°€ì§€ê³  ìˆëŠ”ë° ë²”ìœ„ê°€ ë‹¤ë¥¸ ê²ƒ. (ìŒì–‘ì˜ ë°©í–¥ë„ ë‹¤ë¦„)
sklearn apiì²˜ëŸ¼ ë‚˜ëˆ„ê¸° ìœ„í•´ì„œ -1ê³¼ 1ì‚¬ì´ë¡œ ë§Œë“¤ì–´ì¡Œê³ , ë‚˜ëˆ„ëŠ” ê¸°ì¤€ì€ 0ì„ (0.5ê°€ ì•„ë‹ˆë¼ 0ì¸ ì´ìœ ëŠ” offset_ì„ ë¹¼ì¤˜ì„œ ì¡°ì •í•´ì¤Œ)
**êµ¬ë¶„ ë°©ë²•**  
-1ê³¼ ê°€ê¹Œìš¸ìˆ˜ë¡ outlier, 1ê³¼ ê°€ê¹Œìš¸ìˆ˜ë¡ inlier ê¸°ì¤€ìœ¼ë¡œ ë‚˜ëˆ„ê¸°

### `score_samples(X)`ë‘ `decision_function(X)` ë¹„êµ
- `decision_function()` = `score_samples()` - `offset_`
- `offset_`: `contamination`='auto'ë©´ 0.5ë¡œ ê³ ì •. 0.5ê°€ ì•„ë‹Œ ë‹¤ë¥¸ ìˆ˜ë¼ë©´ ê³ ì •ëœê±´ ì•„ë‹˜ (ë¶„í¬ ì¡°ì •ì„ ìœ„í•´ í•„ìš”í•œë“¯)
  Offset used to define the decision function from the raw scores.
  (in sklearn documents)
  We have the relation: decision_function = score_samples - offset_. offset_ is defined as follows.
  When the contamination parameter is set to â€œautoâ€, the offset is equal to -0.5 as the scores of inliers are
  close to 0 and the scores of outliers are close to -1. When a contamination parameter different than â€œautoâ€ is provided,
  the offset is defined in such a way we obtain the expected number of outliers (samples with decision function < 0) in training.
  
### `score_samples(X)`ë‘ `decision_function(X)` ê·¸ë˜í”„ë¡œ ë¹„êµí•´ë³´ê¸°
- ë²”ìœ„ëŠ” ë‹¤ë¥´ì§€ë§Œ ë¶„í¬ëŠ” ê°™ìŒ.  
- ìŒìˆ˜ë¡œ ì”Œì›Œì„œ original paperì˜ í‘œí˜„ê³¼ ê°™ê²Œ ë§Œë“¤ì–´ì¤Œ
- contamination='auto'ì¼ ë•Œ 
  ![image](https://github.com/juyeon999/Paper_review/assets/132811616/a928d271-ce14-4467-87c3-b4568176a9fe)
  
  ``` python
  d=3
  s=1050
  data = make_gaussian_data(d, s, min_distance=1, seed=2345)
  
  X = np.array(data.iloc[:, :-1])
  y = np.array(data.iloc[:, -1])
  clf = IsolationForest(n_estimators=100, max_samples=256, contamination='auto')
  y_pred = clf.fit(X).predict(X)
  
  score = -clf.score_samples(X)
  dec = -clf.decision_function(X)
  
  # cutoffê°€ ì €ê²Œ ë§ë‚˜? -> ë§ìŒ
  sum(score > -clf.offset_), sum(dec > 0), sum(y_pred == -1) # (87, 87, 87)
  ```
- contamination='0.05'ì¼ë•Œ
   score_samplesì—ì„œ cutoffëŠ” `self.offset_`ì„  
  ![image](https://github.com/juyeon999/Paper_review/assets/132811616/46efe8ca-6b4c-4a2a-99fb-3c1f8e326505)

  
  

### Reference
- score_sample, decision_function ì°¨ì´ê°€ ë­ê³  offset_ì€ ë­”ê°€ìš©??  
  https://stackoverflow.com/questions/68061530/what-is-the-difference-between-decision-function-and-score-samples-in-isolation
